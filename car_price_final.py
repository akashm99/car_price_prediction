# -*- coding: utf-8 -*-
"""Car_price_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14f7NC7lH933-gLm22mxk6QFJ871IIyzv
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
import seaborn as sns
sns.set()

raw_data = pd.read_csv('/content/car_price.csv')

raw_data.head()

raw_data.describe(include='all')

raw_data.isnull().sum()

print(raw_data[['Brand','Price']].groupby(['Brand'], as_index=False).mean())

print(raw_data[['Body','Price']].groupby(['Body'], as_index=False).mean())

print(raw_data[['Engine Type','Price']].groupby(['Engine Type'], as_index=False).mean())

print(raw_data[['Registration','Price']].groupby(['Registration'], as_index=False).mean())

data_no_mv= raw_data.dropna(axis=0)
data_no_mv.describe(include='all')

sns.distplot(data_no_mv['Price'])

q= data_no_mv['Price'].quantile(0.95)
data1= data_no_mv[data_no_mv['Price']<q]

sns.distplot(data1['Price'])

sns.distplot(data1['Mileage'])

q= data1['Mileage'].quantile(0.99)
data2= data1[data1['Mileage']<q]

sns.distplot(data2['EngineV'])

data3=data2[data2['EngineV']<6.5]

sns.distplot(data3['Year'])

q= data3['Year'].quantile(0.01)
data4= data3[data3['Year']>q]

current_year=2020
data4['Year_count']=data4['Year'] - current_year

data_cleaned=data4.reset_index(drop=True)

data_cleaned.describe(include='all')

dfcbsdyj{]9089}\\\\()

f, (ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,3))
ax1.scatter(data_cleaned['Year_count'], data_cleaned['Price'])
ax2.scatter(data_cleaned['EngineV'], data_cleaned['Price'])
ax3.scatter(data_cleaned['Mileage'], data_cleaned['Price'])

sns.distplot(data_cleaned['Price'])

log_price= np.log(data_cleaned['Price'])
data_cleaned['log_price']=log_price

f, (ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,3))
ax1.scatter(data_cleaned['Year_count'], data_cleaned['log_price'])
ax2.scatter(data_cleaned['EngineV'], data_cleaned['log_price'])
ax3.scatter(data_cleaned['Mileage'], data_cleaned['log_price'])

data_cleaned=data_cleaned.drop(['Price','Year'],axis=1)

data_cleaned.columns.values

from statsmodels.stats.outliers_influence import variance_inflation_factor
variables = data_cleaned[['Mileage', 'Year_count', 'EngineV']]
vif=pd.DataFrame()
vif['VIF']=[variance_inflation_factor(variables.values,i) for i in range(variables.shape[1])]
vif['Features']=variables.columns
vif

import statsmodels.api as sm
x=data_cleaned[['Mileage','Year_count','EngineV']]
y=data_cleaned['log_price']

x=sm.add_constant(x)
model= sm.OLS(y,x).fit()

model.summary()

x.iloc[:,1:].corr()

for col in data_cleaned.columns :
  print(col, ':' ,len(data_cleaned[col].unique()))

data_cleaned.Model.value_counts().sort_values(ascending=False).head(50)

top_50= [x for x in data_cleaned.Model.value_counts().sort_values(ascending=False).head(50).index]
top_50

data_model= pd.DataFrame()
for label in top_50:
  data_model[label] = np.where(data_cleaned['Model']==label,1,0)
#data_cleaned[['Model']+top_50]
data_model

data_model.shape

data_clean=data_cleaned.drop(['Brand','Model'], axis=1)
data_clean

data_dummies= pd.get_dummies(data_clean,drop_first=True)
data_dummies

data_with_dummie=pd.concat([data_dummies,data_model], axis=1)
data_with_dummie

data_with_dummie.columns.values

cols=['log_price', 'Mileage', 'EngineV', 'Year_count', 'Body_hatch',
       'Body_other', 'Body_sedan', 'Body_vagon', 'Body_van',
       'Engine Type_Gas', 'Engine Type_Other', 'Engine Type_Petrol',
       'Registration_yes', 'E-Class', 'Vito', 'A6', 'Kangoo', 'Camry',
       'Caddy', 'X5', 'Megane', 'Land Cruiser Prado', '520', 'Trafic',
       'Passat B6', 'Polo', 'Touareg', '320', 'T5 (Transporter)',
       'Passat B5', 'A4', 'C-Class', 'Q7', '525', 'Rav 4', 'Lancer',
       '530', 'Lancer X', 'Passat B7', 'Pajero Wagon', 'Corolla', 'Jetta',
       'T4 (Transporter)', 'S 500', 'T5 (Transporter) ', 'Avensis', 'X6',
       '318', 'Auris', 'Laguna', 'Outlander', 'Galant', 'Pajero Sport',
       'A8', 'Golf IV', '730', 'Outlander XL', 'Scenic', 'A6 Allroad',
       'ML 350', 'Passat CC', 'Touran', 'Multivan']

data_preprocessed= data_with_dummie[cols]
data_preprocessed

targets=data_preprocessed['log_price']
inputs = data_preprocessed.drop(['log_price'],axis=1)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(inputs)

inputs_scaled = scaler.transform(inputs)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=365)

reg= LinearRegression()
reg.fit(x_train,y_train)

yhat = reg.predict(x_train)

plt.scatter(y_train, yhat)
plt.xlim(6,13)
plt.ylim(6,13)

sns.distplot(y_train-yhat)

r2 = reg.score(x_train,y_train)
r2

n= x_train.shape[0]
p= x_train.shape[1]
adj_r2=1 - (1- r2) * (n-1)/(n-p-1)
adj_r2

score=pd.DataFrame()
score['Variable']=['r2','adj r2']
score['Score']=(round(float(r2)*100,2) , round(float(adj_r2)*100,2))
score

reg.intercept_

reg.coef_

reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])
reg_summary['Weights']= reg.coef_
reg_summary

yhat_test=reg.predict(x_test)
plt.scatter(y_test,yhat_test, alpha=0.2)
plt.xlim(6,13)
plt.ylim(6,13)

reg.score(x_test,y_test)

df_pf=pd.DataFrame(np.exp(yhat_test), columns=['Predictions'])
df_pf['Targets']=np.exp(y_test)
df_pf

y_test

y_test=y_test.reset_index(drop=True)

y_test

#df_pf=pd.DataFrame(np.exp(yhat_test), columns=['Predictions'])
df_pf['Targets']=np.exp(y_test)
df_pf

df_pf['Residual']=df_pf['Targets']- df_pf['Predictions']
df_pf

df_pf['Difference%']= np.absolute(df_pf['Residual']/df_pf['Targets']*100)
df_pf

df_pf.describe()

#pd.options.display.max_rows=999
#pd.set_option('display.float_format', lambda x: '%.2f' %x)
#df_pf.sort_values(by=['Difference%'])

